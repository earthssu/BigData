# Start Hadoop
- test를 위해 단일 노드로 hadoop system을 키는 것을 수행해볼 것이다.
- 리눅스를 거의 처음 써보는데 hadoop부터 깔려고 하니까 매우 막막했고, 삽질 겁나 많이 했다.
- 어떻게든 하면 되는 거 같다.


### 참고
[Apache 공식 홈페이지](https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-common/SingleCluster.html)

- EC2는 t2.micro 절대 안되는 거 같다.(EC2 쓸바에는 EMR써서 편하게 하자.)
- 앵간하면 성능 좋은 거 쓰도록하자.
- 컴퓨터 성능이 좋다면, 그냥 VM깔고 쓰도록하자.

### Version
- Hadoop 2.7.7
- JAVA 8
- Ubuntu 16.04

### 순서
1. 각종 Linux 실행 방식을 동원하여 Ubuntu 16.04를 실행시킨다.
2. 유저 생성
```bash
$sudo su // sudo user로 전환
$adduser hadoop // 비밀번호만 잘 기억하고 나머지는 enter
$usermod -a -G sudo hadoop //sudo 권한 부여 (ec2는 안됨)
```
3. zshell 설치(그냥 편리하게 쓸려고 설치 의미는 없음)
```bash
$apt-get install git
$apt-get install zsh
$curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh
```

4. JAVA 설치
```bash
$add-apt-repository -y ppa:webupd8team/java
$apt-get update
$apt install openjdk-8-jre-headless
$echo JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64" >> /etc/environment //환경변수 설정 잘 안되면 vi /etc/environment로 직접가서 하자.
$source /etc/environment //적용
$echo $JAVA_HOME //확인
```

5. hadoop 설치
```bash
$apt-get -y install vim build-essential
$wget http://apache.mirror.cdnetworks.com/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz
$mv -r hadoop-2.7.7.tar.gz /home/hadoop/ //hadoop root 폴더로 이동
$cd /home/hadoop
$tar -cvzf hadoop-2.7.7.tar.gz //압축해제
```

6. hadoop 기본 setting
  - core-site.xml
  ```bash
  $cd /home/hadoop/hadoop-2.7.7
  $vi etc/hadoop/core-site.xml
  ```

  ```
  <configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
  </configuration>
  ```

  - hdfs-site.xml
  ```bash
  $cd /home/hadoop/hadoop-2.7.7
  $vi etc/hadoop/hdfs-site.xml
  ```

  ```
  <configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
  </configuration>
  ```

7. yarn 기본 setting
- mapred-site.xml
```bash
$cd /home/hadoop/hadoop-2.7.7
$vi etc/hadoop/mapred-site.xml
```

```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

- yarn-site.xml
```bash
$cd /home/hadoop/hadoop-2.7.7
$vi etc/hadoop/yarn-site.xml
```

```
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

8. password없이 hadoop 사용하기.(ec2 불가능)
```bash
$ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$chmod 0600 ~/.ssh/authorized_keys
```

9. 실행하기.
```bash
$cd /home/hadoop/hadoop-2.7.7/
$bin/hdfs namenode -format // namenode 처음 format 생성
$sbin/start-dfs.sh // hdfs 시작
$sbin/start-yarn.sh // yarn 시작
$jps // 현재 제대로 실행되었는지 확인.(총 6개가 떠야함 namenode, datanode, secondarynamenode, jps, resourcemanager, nodemanager)
$bin/hdfs dfs -mkdir /user // directory 생성 root 폴더는 /이다.
$bin/hdfs dfs -put etc/hadoop input // local의 file을 hdfs로 전송
$bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-example-2.7.7.jar grep input output 'dfs[a-z.]+' // mapreduce example 수행
$bin/hdfs dfs -get output output // hdfs로 부터 local로 file을 불러옴.
$cat output/* // output을 종합.
$bin/hdfs dfs -cat output/* // output을 종합 결과는 동일.
$sbin/stop-yarn.sh // yarn 종료
$sbin/stop-dfs.sh // hdfs 종료
```
